Switch to EfficientNet-b7 and also follow other previous plans. When switiching,
implement a way to compare training time, testing time and testing accuracy with EfficientNet-b0
Note, when switiching to EfficientNet-b7, must remember to download the correct 
pretrained weights

Take heed of the user warnings from trainer.run(train_loader, max_epochs=num_epochs) if things don't work out. If things do 
work out, make another file that takes heed of them, and see if things still wor out. User warnings (note, for the first one, 
the webpage I am adapting code from gets the same UserWarning, but it seems they haven't done so much about it so it may not 
really matter)

/home/james/anaconda3/envs/pytorch/lib/python3.9/site-packages/ignite/contrib/handlers/base_logger.py:96: UserWarning: Provided metric name 'batchloss' is missing in engine's state metrics: []
  warnings.warn(

home/james/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "

Remember, model used finally is initialised using apex.amp.initialize() at the moment, and you may want to change that to torch amp.

Also, may want to implement a way to save weights after training, so you don't have to train everytime you want to test.

If everything doesn't simply work out with the testing etc in the next run, going to find a way to save trained data then continue.